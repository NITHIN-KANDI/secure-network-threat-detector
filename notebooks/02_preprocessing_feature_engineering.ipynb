{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f60129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "PROC_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "os.makedirs(PROC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e03aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load raw training data\n",
    "train_path = os.path.join(RAW_DIR, 'UNSW_NB15_training-set.csv')\n",
    "df = pd.read_csv(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43df0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop integrity columns if present\n",
    "df = df.drop(columns=[c for c in ['row_hash', 'attack_cat'] if c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af118ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Separate features and label\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "042b34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train/test split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82ed4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define feature groups\n",
    "# Continuous ratelike features\n",
    "continuous = ['dur', 'rate', 'sload']\n",
    "# Count-like and depth features\n",
    "counts = ['spkts','dpkts','sbytes','dbytes','trans_depth','response_body_len']\n",
    "# Window counts\n",
    "window_ct = ['ct_src_dport_ltm','ct_dst_sport_ltm']\n",
    "# Numeric other\n",
    "numeric_other = ['ct_ftp_cmd']\n",
    "# Categorical\n",
    "categorical = ['proto','service','state','ct_flw_http_mthd']\n",
    "# Binary flags\n",
    "binary = ['is_ftp_login','is_sm_ips_ports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3278014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build preprocessing pipelines\n",
    "log_tf = FunctionTransformer(np.log1p, validate=False)\n",
    "cont_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "cnt_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('log',   log_tf),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Remove sparse parameter\n",
    "])\n",
    "bin_pipe = Pipeline([('passthrough', 'passthrough')])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cont', cont_pipe, continuous),\n",
    "    ('cnt',  cnt_pipe, counts + window_ct + numeric_other),\n",
    "    ('cat',  cat_pipe, categorical),\n",
    "    ('bin',  bin_pipe, binary)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ff3afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Fit and transform training data\n",
    "# Ensure we only pass the expected feature columns in the correct order\n",
    "feature_cols = continuous + counts + window_ct + numeric_other + categorical + binary\n",
    "X_train_proc = preprocessor.fit_transform(X_train_raw[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d637dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Balance training set if needed Balance training set if needed\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train_proc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aea1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Fit PCA to reduce dimensionality\n",
    "pca = PCA(n_components=10, random_state=42)  # Specify exact number of components\n",
    "X_train_pca = pca.fit_transform(X_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ce75dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\dorai\\\\OneDrive\\\\Documents\\\\Documents\\\\SEM6\\\\Computer Security\\\\Project_cs\\\\IDS-binary-classification\\\\data\\\\processed\\\\pca.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. Persist artifacts\n",
    "joblib.dump(preprocessor, os.path.join(PROC_DIR, 'preprocessor.pkl'))\n",
    "joblib.dump(pca,         os.path.join(PROC_DIR, 'pca.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cde2c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 2 complete: preprocessor.pkl and pca.pkl saved.\n"
     ]
    }
   ],
   "source": [
    "# 11. Optionally, transform test set and save for Module 3\n",
    "df_test_proc = preprocessor.transform(X_test_raw)\n",
    "np.save(os.path.join(PROC_DIR, 'X_test_proc.npy'), df_test_proc)\n",
    "pd.DataFrame({'label': y_test}).to_csv(os.path.join(PROC_DIR, 'y_test.csv'), index=False)\n",
    "\n",
    "print(\"Module 2 complete: preprocessor.pkl and pca.pkl saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "769b82fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
